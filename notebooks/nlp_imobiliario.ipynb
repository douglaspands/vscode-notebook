{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP - Mercado Imobiliario\n",
    "\n",
    "Após instalar as dependencias, executar a instalação do modelo:\n",
    "\n",
    "```sh\n",
    "python -m spacy download pt_core_news_md\n",
    "```\n",
    "\n",
    "O modelo instalado será o menor, porem existe modelos maiores. Abaixo segue a ordem do menor para o maior:\n",
    "- pt_core_news_sm\n",
    "- pt_core_news_md\n",
    "- pt_core_news_lg\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Inicial\n",
    "\n",
    "### Termos do mercado imobiliario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "termos_imobiliarios = [\n",
    "    \"imovel\",\n",
    "    \"quartos\",\n",
    "    \"dormitorios\",\n",
    "    \"suite\",\n",
    "    \"banheiros\",\n",
    "    \"vagas\",\n",
    "    \"garagem\",\n",
    "    \"area\",\n",
    "    \"sala\",\n",
    "    \"cozinha\",\n",
    "    \"varanda\",\n",
    "    \"churrasqueira\",\n",
    "    \"piscina\",\n",
    "    \"academia\",\n",
    "    \"condominio\",\n",
    "    \"bairro\",\n",
    "]\n",
    "\n",
    "de_para_imobiliario = {\n",
    "    \"dorm\": \"dormitorio\",\n",
    "    \"dorms\": \"dormitorios\",\n",
    "}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funções de correção e NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_correcao(frase: str) -> str:\n",
    "    def remover_acentos(texto: str) -> str:\n",
    "        import unicodedata\n",
    "\n",
    "        nfkd_form = unicodedata.normalize(\"NFKD\", texto)\n",
    "        only_ascii = nfkd_form.encode(\"ASCII\", \"ignore\")\n",
    "        return only_ascii.decode(\"utf8\")\n",
    "\n",
    "    import re\n",
    "\n",
    "    frase = remover_acentos(frase)\n",
    "    frase = re.sub(r\"([0-9]+)\\s*mil\\s\", r\"\\1.000 \", frase)\n",
    "    frase = re.sub(r\"([0-9]+)\\s*milhoes\\s\", r\"\\1.000.000 \", frase)\n",
    "\n",
    "    palavras = re.split(r'\\s+', frase.lower())\n",
    "    palavras_imobiliaria = []\n",
    "    for palavra in palavras:\n",
    "        achou = False\n",
    "        palavra_ = palavra.strip()\n",
    "        for de, para in de_para_imobiliario.items():\n",
    "            if palavra_ in de.split(\",\"):\n",
    "                palavras_imobiliaria.append(para)\n",
    "                achou = True\n",
    "                break\n",
    "        if not achou:\n",
    "            palavras_imobiliaria.append(palavra_)\n",
    "    return \" \".join(palavras_imobiliaria)\n",
    "\n",
    "\n",
    "def corrigir_ortografia(frase: str) -> str:\n",
    "    import spellchecker\n",
    "\n",
    "    corretor = spellchecker.SpellChecker(language=\"pt\")\n",
    "    palavras = frase.split()\n",
    "    palavras_corrigidas = []\n",
    "    for palavra in palavras:\n",
    "        if correcao := corretor.correction(palavra):\n",
    "            palavras_corrigidas.append(correcao)\n",
    "        else:\n",
    "            palavras_corrigidas.append(palavra)\n",
    "    frase_corrigida = \" \".join(palavras_corrigidas)\n",
    "    return frase_corrigida\n",
    "\n",
    "\n",
    "def extrair_tokens(frase: str) -> list[str]:\n",
    "    import spacy\n",
    "\n",
    "    nlp = spacy.load(\"pt_core_news_md\")\n",
    "    doc = nlp(frase)\n",
    "    tokens = []\n",
    "    for palavra in doc:\n",
    "        if (\n",
    "            palavra.text.lower() in termos_imobiliarios\n",
    "            or palavra.like_num\n",
    "            or palavra.pos_ in [\"PROPN\", \"NOUN\"]\n",
    "        ):\n",
    "            tokens.append(palavra.text)\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def extrair_chunks(frase: str) -> list[str]:\n",
    "    import spacy\n",
    "\n",
    "    nlp = spacy.load(\"pt_core_news_md\")\n",
    "    doc = nlp(frase)\n",
    "    chunks = []\n",
    "    for chunk in doc.noun_chunks:\n",
    "        chunks.append(chunk.text)\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def retirar_verbos(frase: str) -> tuple[str, list[str]]:\n",
    "    import spacy\n",
    "\n",
    "    nlp = spacy.load(\"pt_core_news_md\")\n",
    "    doc = nlp(frase)\n",
    "    palavras_sem_verbos = []\n",
    "    tokens_sem_verbos = []\n",
    "    for palavra in doc:\n",
    "        if palavra.pos_ != \"VERB\":\n",
    "            palavras_sem_verbos.append(palavra.text)\n",
    "            tokens_sem_verbos.append(palavra.text)\n",
    "    frase_sem_verbos = \" \".join(palavras_sem_verbos)\n",
    "    return frase_sem_verbos, tokens_sem_verbos\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execução"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frase corrigida: quero um imóvel 4 dormitórios e 2 banheiros custando no maximo 2.000.000 reais\n",
      "Tokens: ['um', 'imóvel', '4', 'dormitórios', '2', 'banheiros', '2.000.000', 'reais']\n",
      "Chunks: ['um imóvel', '4 dormitórios', '2 banheiros', '2.000.000 reais']\n",
      "Frase sem verbos: um imóvel 4 dormitórios e 2 banheiros no maximo 2.000.000 reais\n",
      "Tokens sem verbos: ['um', 'imóvel', '4', 'dormitórios', 'e', '2', 'banheiros', 'no', 'maximo', '2.000.000', 'reais']\n"
     ]
    }
   ],
   "source": [
    "# Testar as funções com uma frase de exemplo\n",
    "# frase = pre_correcao(\"queru um imovel 4 dorms\")\n",
    "frase = pre_correcao(\"quero um imóvel 4 dormitórios e 2 banheiros custando no maximo 2 milhões reais\")\n",
    "frase_corrigida = corrigir_ortografia(frase)\n",
    "print(\"Frase corrigida:\", frase_corrigida)\n",
    "tokens = extrair_tokens(frase_corrigida)\n",
    "print(\"Tokens:\", tokens)\n",
    "chunks = extrair_chunks(frase_corrigida)\n",
    "print(\"Chunks:\", chunks)\n",
    "frase_sem_verbos, tokens_sem_verbos = retirar_verbos(frase_corrigida)\n",
    "print(\"Frase sem verbos:\", frase_sem_verbos)\n",
    "print(\"Tokens sem verbos:\", tokens_sem_verbos)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk: um imóvel 1 3\n",
      "chunk: 4 dormitórios 3 5\n",
      "chunk: 2 banheiros 6 8\n",
      "chunk: maximo 500 mil reais 10 14\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "[E102] Can't merge non-disjoint spans. 'mil' is already part of tokens to merge. If you want to find the longest non-overlapping spans, you can use the util.filter_spans helper:\nhttps://spacy.io/api/top-level#util.filter_spans",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[57], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mretokenize:\u001b[39m\u001b[39m'\u001b[39m, token\u001b[39m.\u001b[39mtext, token\u001b[39m.\u001b[39mpos_)\n\u001b[1;32m     18\u001b[0m \u001b[39m# teste_01('O carro custa 20 mil reais')\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m teste_01(\u001b[39m'\u001b[39;49m\u001b[39mquero um imóvel 4 dormitórios e 2 banheiros custando no maximo 500 mil reais\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[57], line 14\u001b[0m, in \u001b[0;36mteste_01\u001b[0;34m(frase)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[39mif\u001b[39;00m token\u001b[39m.\u001b[39mlike_num:\n\u001b[1;32m     13\u001b[0m             next_token \u001b[39m=\u001b[39m doc[token\u001b[39m.\u001b[39mi \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m]\n\u001b[0;32m---> 14\u001b[0m             retokenizer\u001b[39m.\u001b[39;49mmerge(doc[token\u001b[39m.\u001b[39;49mi: next_token\u001b[39m.\u001b[39;49mi \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m])\n\u001b[1;32m     15\u001b[0m \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m doc:\n\u001b[1;32m     16\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mretokenize:\u001b[39m\u001b[39m'\u001b[39m, token\u001b[39m.\u001b[39mtext, token\u001b[39m.\u001b[39mpos_)\n",
      "File \u001b[0;32m~/workspace/vscode-notebook/.venv/lib/python3.10/site-packages/spacy/tokens/_retokenize.pyx:58\u001b[0m, in \u001b[0;36mspacy.tokens._retokenize.Retokenizer.merge\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: [E102] Can't merge non-disjoint spans. 'mil' is already part of tokens to merge. If you want to find the longest non-overlapping spans, you can use the util.filter_spans helper:\nhttps://spacy.io/api/top-level#util.filter_spans"
     ]
    }
   ],
   "source": [
    "def teste_01(frase):\n",
    "    import spacy\n",
    "    \n",
    "    nlp = spacy.load(\"pt_core_news_md\")\n",
    "    doc = nlp(frase)\n",
    "    for entity in doc.ents:\n",
    "        print('ent:', entity, entity.label_)\n",
    "    for chunk in doc.noun_chunks:\n",
    "        print('chunk:', chunk.text, chunk.start, chunk.end)\n",
    "    with doc.retokenize() as retokenizer:\n",
    "        for token in doc:\n",
    "            if token.like_num:\n",
    "                next_token = doc[token.i + 1]\n",
    "                retokenizer.merge(doc[token.i: next_token.i + 1])\n",
    "    for token in doc:\n",
    "        print('retokenize:', token.text, token.pos_)\n",
    "\n",
    "# teste_01('O carro custa 20 mil reais')\n",
    "teste_01('quero um imóvel 4 dormitórios e 2 banheiros custando no maximo 500 mil reais')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
