{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ollama + LangChain\n",
    "\n",
    "Usar a API do Ollama com o biblioteca LangChain.\n",
    "\n",
    "Como instalar:\n",
    "\n",
    "```sh\n",
    "pip install langchain-ollama\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM - Enum e Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Protocol, Iterator\n",
    "from enum import StrEnum\n",
    "\n",
    "\n",
    "class LLMEnum(StrEnum):\n",
    "    LAMMA3_1 = \"llama3.1:8b\"\n",
    "    LAMMA3_2 = \"llama3.2:3b\"\n",
    "    LAMMA3_2_VISION = \"llama3.2-vision:11b\"\n",
    "    GEMMA2_2 = \"gemma2:2b\"\n",
    "    GEMMA2_9 = \"gemma2:9b\"\n",
    "    PHI3_5 = \"phi3.5:3.8b\"\n",
    "    LLAVA = \"llava:7b\"\n",
    "\n",
    "\n",
    "class LLMInterface(Protocol):\n",
    "    def question(self, question: str) -> str: ...\n",
    "    def question_s(self, question: str) -> Iterator[str]: ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM - Implementação"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "from typing import Iterator\n",
    "\n",
    "\n",
    "class LLMPrompt(LLMInterface):\n",
    "    MODEL = LLMEnum.LAMMA3_2\n",
    "    TEMPLATE = \"\"\"\n",
    "        Pergunta: {question}\n",
    "        \n",
    "        Resposta: Me explique com calma e passo a passo.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        prompt = ChatPromptTemplate.from_template(self.TEMPLATE)\n",
    "        model = OllamaLLM(model=self.MODEL)\n",
    "        self._chain = prompt | model\n",
    "\n",
    "    def question(self, question: str) -> str:\n",
    "        return self._chain.invoke({\"question\": question})\n",
    "\n",
    "    def question_s(self, question: str) -> Iterator[str]:\n",
    "        return self._chain.stream({\"question\": question})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retornando uma vez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Claro, vou explicar o que é LangChain de forma clara e detalhada.\n",
       "\n",
       "**O que é LangChain?**\n",
       "\n",
       "LangChain é uma biblioteca de Python para NLP (Inteligência Artificial da Linguagem) projetada especificamente para criar modelos de linguagem em grande escala. Ela foi criada com a intenção de fornecer uma solução fácil de usar e escalável para trabalhar com grandes conjuntos de texto, como documentos, artigos, livros, etc.\n",
       "\n",
       "**Como funciona LangChain?**\n",
       "\n",
       "LangChain é baseada na ideia de criar um modelo de linguagem que possa aprender e entender o significado das palavras, frases e sentenças. Isso é alcançado utilizando técnicas de aprendizado de máquina (Machine Learning) e processamento de linguagem natural.\n",
       "\n",
       "Aqui está uma explicação passo a passo sobre como funciona LangChain:\n",
       "\n",
       "1. **Preprocessamento**: O primeiro passo é preparar os dados para serem processados. Isso inclui dividir os textos em palavras, remover stopwords (palavras comuns que não adicionam muito valor à mensagem) e normalizar o texto.\n",
       "2. **Representação de dados**: Em seguida, os dados são representados como um vetor de características. Isso é feito utilizando técnicas de processamento de linguagem natural, como tokenização, partes de speech e embeddings (vetores que capturam o significado das palavras).\n",
       "3. **Aprendizado de máquina**: O modelo de linguagem aprende a relacionar as representações de dados com as respostas desejadas. Isso é feito utilizando técnicas de aprendizado de máquina, como regression ou classificação.\n",
       "4. **Interação com o usuário**: Uma vez que o modelo de linguagem esteja treinado, pode ser utilizado para responder a perguntas ou fornecer informações sobre um texto específico.\n",
       "\n",
       "**Vantagens de LangChain**\n",
       "\n",
       "LangChain oferece várias vantagens em relação às outras bibliotecas de NLP disponíveis no mercado. Algumas das principais vantagens incluem:\n",
       "\n",
       "* **Escala**: LangChain é projetada para trabalhar com grandes conjuntos de texto, tornando-a uma excelente escolha para aplicações que requerem processamento de linguagem natural em larga escala.\n",
       "* **Facilidade de uso**: A biblioteca é fácil de usar e requer apenas alguns passos para configurar o modelo de linguagem.\n",
       "* **Flexibilidade**: LangChain permite que os usuários personalizem a biblioteca para atender às suas necessidades específicas.\n",
       "\n",
       "**Conclusão**\n",
       "\n",
       "LangChain é uma biblioteca poderosa e escalável de NLP que oferece uma solução fácil de usar para trabalhar com grandes conjuntos de texto. Com sua capacidade de aprender e entender o significado das palavras, frases e sentenças, LangChain pode ser utilizado em uma variedade de aplicações, desde a geração de texto automático até a análise de sentimentos e a tradução de idiomas."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "prompt = LLMPrompt()\n",
    "\n",
    "question = \"O que é LangChain?\"\n",
    "response = prompt.question(question=question)\n",
    "Markdown(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retornando via streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Claro, vou explicar o que é LangChain de forma clara e detalhada.\n",
       "\n",
       "**O que é LangChain?**\n",
       "\n",
       "LangChain é uma biblioteca de Python que permite criar modelos de linguagem mais complexos e escaláveis. Ela fornece uma ferramenta para trabalhar com grandes sequências de texto, como documentos, artigos e livros.\n",
       "\n",
       "**Passo a passo:**\n",
       "\n",
       "1. **Instalação**: Primeiramente, você precisa instalar o LangChain em seu ambiente Python. Isso pode ser feito usando o comando `pip install langchain`.\n",
       "2. **Importação**: Após a instalação, você importa o módulo LangChain em seu script ou programa.\n",
       "3. **Criar um modelo de linguagem**: Para criar um modelo de linguagem com LangChain, você precisa criar uma classe que extenda a classe `LangChain`. Essa classe deve conter métodos para processar sequências de texto e fazer inferências baseadas nesse texto.\n",
       "4. **Carregar dados**: Você precisa carregar seus dados de entrada (sequências de texto) em um formato compatível com LangChain, como uma lista ou um array.\n",
       "5. **Treinar o modelo**: Com os dados carregados, você pode treinar seu modelo de linguagem. Isso envolve processar as sequências de texto e fazer inferências sobre elas.\n",
       "6. **Fazer inferências**: Após o treinamento, você pode usar o modelo para fazer inferências baseadas nas sequências de texto que você carregou.\n",
       "\n",
       "**Exemplo básico:**\n",
       "\n",
       "Aqui está um exemplo básico de como criar um modelo de linguagem com LangChain:\n",
       "```python\n",
       "import langchain\n",
       "\n",
       "# Crie uma classe que extenda a classe LangChain\n",
       "class MeuModelo(LangChain):\n",
       "    def __init__(self, dados):\n",
       "        super().__init__()\n",
       "        self.dados = dados\n",
       "    \n",
       "    # Defina um método para processar sequências de texto\n",
       "    def processar(self, sequencia):\n",
       "        # Faça algo com a sequência de texto aqui...\n",
       "        return sequencia.upper()\n",
       "\n",
       "# Crie uma lista de sequências de texto\n",
       "sequencias = [\"Olá, como você está?\", \"Eu estou bem, obrigado\", \"Agradável conhecer você\"]\n",
       "\n",
       "# Crie um modelo de linguagem\n",
       "modelo = MeuModelo(sequencias)\n",
       "\n",
       "# Treine o modelo\n",
       "modelo.treinar()\n",
       "\n",
       "# Fazendo inferências com o modelo\n",
       "inference = modelo.processar(\"Quem é esse homem?\")\n",
       "print(inference)  # SAO O MECANICO DO MEU VEHICLE\n",
       "```\n",
       "Esse é apenas um exemplo básico, mas isso deve dar uma ideia de como funciona LangChain.\n",
       "\n",
       "**Conclusão:**\n",
       "\n",
       "LangChain é uma ferramenta poderosa para criar modelos de linguagem mais complexos e escaláveis. Com ela, você pode processar sequências de texto e fazer inferências baseadas nesses textos. É importante notar que o desenvolvimento de um modelo de linguagem envolve muitos passos e pode ser um processo complexo. No entanto, LangChain pode ajudar a tornar esse processo mais fácil e eficiente.\n",
       "\n",
       "Espero que isso tenha ajudado! Se tiver alguma dúvida adicional, sinta-se à vontade para perguntar."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython import display\n",
    "from io import StringIO\n",
    "\n",
    "prompt = LLMPrompt()\n",
    "\n",
    "question = \"O que é LangChain?\"\n",
    "response = StringIO()\n",
    "for stream in prompt.question_s(question=question):\n",
    "    response.write(stream)\n",
    "    display.display(display.Markdown(response.getvalue()), clear=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Chat - Implementação"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from langchain_ollama import ChatOllama\n",
    "from typing import Iterator\n",
    "from io import StringIO\n",
    "\n",
    "\n",
    "class LLMChatPrompt(LLMInterface):\n",
    "    MODEL = LLMEnum.LAMMA3_2\n",
    "    TEMPLATE = [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"Voce é um assistente virtual, responda com calma e passo a passo\",\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    def __init__(self):\n",
    "        model = ChatOllama(model=self.MODEL)\n",
    "        self._chain = model\n",
    "        self._history = self.TEMPLATE.copy()\n",
    "\n",
    "    def question(self, question: str) -> str:\n",
    "        self._history.append((\"human\", question))\n",
    "        response = self._chain.invoke(self._history)\n",
    "        self._history.append((\"human\", response.content))\n",
    "        return response.content\n",
    "\n",
    "    def question_s(self, question: str) -> Iterator[str]:\n",
    "        self._history.append((\"human\", question))\n",
    "        result = StringIO()\n",
    "        for response in self._chain.stream(self._history):\n",
    "            result.write(response.content)\n",
    "            yield response.content\n",
    "        self._history.append((\"human\", result.getvalue()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retornando uma vez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Claro! Vou te explicar de forma detalhada sobre o FastAPI.\n",
       "\n",
       "**O que é o FastAPI?**\n",
       "\n",
       "FastAPI é uma biblioteca de microserviços para Python desenvolvida em asyncio. Foi criada em 2017 e lançada oficialmente em 2020. O objetivo principal do FastAPI é criar APIs rápidas, escaláveis e seguras.\n",
       "\n",
       "**Características principais:**\n",
       "\n",
       "1. **Rápida**: A velocidade de execução é muito rápida, graças ao uso de coroutines e asyncio.\n",
       "2. **Escalável**: O FastAPI é projetado para lidar com uma grande quantidade de tráfego e requisições simultâneas.\n",
       "3. **Seguro**: A biblioteca inclui muitas características de segurança, como autenticação e autorização, para proteger suas APIs contra ataques mal-intencionados.\n",
       "4. **Elegante**: O FastAPI tem uma API simples e fácil de usar, o que torna muito mais atraente para os desenvolvedores Python.\n",
       "\n",
       "**Tipos de APIs suportadas:**\n",
       "\n",
       "1. **APIs RESTful**: Suporte a métodos HTTP comuns (GET, POST, PUT, DELETE).\n",
       "2. **APIs GraphQL**: Suporte a query executivas e respostas JSON.\n",
       "3. **APIs SOAP**: Suporte a protocolo SOAP.\n",
       "\n",
       "**Como funciona?**\n",
       "\n",
       "O FastAPI é projetado para criar microserviços de API, que podem ser facilmente escalados e gerenciados. Ele utiliza um modelo de design em camadas (layers) para separar as responsabilidades das diferentes partes do sistema.\n",
       "\n",
       "1. **Camada de entrada**: Processa a requisição HTTP vinda do cliente.\n",
       "2. **Camada de negócios**: Executa o negócio da aplicação, como processamento de dados ou comunicação com bancos de dados.\n",
       "3. **Camada de output**: Gera as respostas HTTP para o cliente.\n",
       "\n",
       "**Exemplo básico:**\n",
       "\n",
       "Aqui está um exemplo básico de como criar uma API com o FastAPI:\n",
       "```python\n",
       "from fastapi import FastAPI\n",
       "\n",
       "app = FastAPI()\n",
       "\n",
       "@app.get(\"/\")\n",
       "def read_root():\n",
       "    return {\"message\": \"Bem-vindo à API!\"}\n",
       "\n",
       "@app.get(\"/items/{item_id}\")\n",
       "def read_item(item_id: int):\n",
       "    return {\"item_id\": item_id}\n",
       "```\n",
       "Esse código cria uma API com duas rotas: `/` e `/items/{item_id}`. A primeira rota retorna um JSON com uma mensagem de bem-vindo, enquanto a segunda rota retorna o ID do item.\n",
       "\n",
       "Essa é uma visão geral sobre o FastAPI! Se você tiver alguma dúvida específica ou quiser saber mais sobre como usar o FastAPI, sinta-se à vontade para perguntar."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython import display\n",
    "\n",
    "prompt = LLMChatPrompt()\n",
    "\n",
    "question = \"O que é o FastAPI?\"\n",
    "response = prompt.question(question=question)\n",
    "display.display(display.Markdown(response), clear=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Vou te explicar como criar um CRUD (Create, Read, Update e Delete) com o FastAPI!\n",
       "\n",
       "**Criando um CRUD com o FastAPI**\n",
       "\n",
       "Para criar um CRUD, precisamos definir quatro rotas: `POST` para criar um novo item, `GET` para ler todos os items, `PUT` para atualizar um item existente e `DELETE` para apagar um item.\n",
       "\n",
       "Aqui está um exemplo de como criar um CRUD com o FastAPI:\n",
       "```python\n",
       "from fastapi import FastAPI, HTTPException\n",
       "from pydantic import BaseModel\n",
       "\n",
       "app = FastAPI()\n",
       "\n",
       "# Definindo a estrutura do item\n",
       "class Item(BaseModel):\n",
       "    id: int\n",
       "    title: str\n",
       "    description: str\n",
       "\n",
       "# Criando uma lista para armazenar os items\n",
       "items = []\n",
       "\n",
       "@app.get(\"/items/\")\n",
       "async def read_items():\n",
       "    return items\n",
       "\n",
       "@app.post(\"/items/\")\n",
       "async def create_item(item: Item):\n",
       "    item.id = len(items) + 1\n",
       "    items.append(item)\n",
       "    return item\n",
       "\n",
       "@app.get(\"/items/{item_id}\")\n",
       "async def read_item(item_id: int):\n",
       "    for item in items:\n",
       "        if item.id == item_id:\n",
       "            return item\n",
       "    raise HTTPException(status_code=404, detail=\"Item não encontrado\")\n",
       "\n",
       "@app.put(\"/items/{item_id}\")\n",
       "async def update_item(item_id: int, item: Item):\n",
       "    for existing_item in items:\n",
       "        if existing_item.id == item_id:\n",
       "            existing_item.title = item.title\n",
       "            existing_item.description = item.description\n",
       "            return existing_item\n",
       "    raise HTTPException(status_code=404, detail=\"Item não encontrado\")\n",
       "\n",
       "@app.delete(\"/items/{item_id}\")\n",
       "async def delete_item(item_id: int):\n",
       "    for index, item in enumerate(items):\n",
       "        if item.id == item_id:\n",
       "            del items[index]\n",
       "            return {\"message\": \"Item excluído com sucesso\"}\n",
       "    raise HTTPException(status_code=404, detail=\"Item não encontrado\")\n",
       "```\n",
       "Esse código cria um CRUD para gerenciar itens de forma básica. Ele usa a estrutura `Item` definida anteriormente para criar um novo item e armazená-lo na lista `items`. As rotas `read_items`, `create_item`, `read_item`, `update_item` e `delete_item` são criadas para realizar as operações CRUD respectivas.\n",
       "\n",
       "**Exemplo de uso:**\n",
       "\n",
       "Aqui está um exemplo de como usar o CRUD:\n",
       "```bash\n",
       "# Criando um novo item\n",
       "curl -X POST \\\n",
       "  http://localhost:8000/items/ \\\n",
       "  -H 'Content-Type: application/json' \\\n",
       "  -d '{\"id\":1,\"title\":\"Item 1\",\"description\":\"Descrição do item 1\"}'\n",
       "\n",
       "# Lendo todos os itens\n",
       "curl http://localhost:8000/items/\n",
       "\n",
       "# Lendo um item específico\n",
       "curl http://localhost:8000/items/1\n",
       "\n",
       "# Atualizando um item existente\n",
       "curl -X PUT \\\n",
       "  http://localhost:8000/items/1 \\\n",
       "  -H 'Content-Type: application/json' \\\n",
       "  -d '{\"id\":1,\"title\":\"Item 1 atualizado\",\"description\":\"Descrição do item 1 atualizado\"}'\n",
       "\n",
       "# Excluindo um item existente\n",
       "curl -X DELETE \\\n",
       "  http://localhost:8000/items/1\n",
       "```\n",
       "Esse é apenas um exemplo básico de como criar um CRUD com o FastAPI! Você pode adicionar mais funcionalidades e melhorar a eficiência do código para atender às suas necessidades específicas."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "question = \"Poderia criar um CRUD?\"\n",
    "response = prompt.question(question=question)\n",
    "display.display(display.Markdown(response), clear=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retornando via streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Vou explicar de forma clara e passo a passo.\n",
       "\n",
       "**O que é o FastAPI?**\n",
       "\n",
       "FastAPI é uma biblioteca Python para criar APIs (Application Programming Interfaces) rápidas, seguras e escaláveis. Foi criado com a missão de fornecer uma API simples e flexível para desenvolver aplicações web modernas.\n",
       "\n",
       "**Características principais do FastAPI:**\n",
       "\n",
       "1. **Desenvolvimento rápido**: FastAPI permite que você crie APIs rapidamente, sem precisar escrever código desnecessário.\n",
       "2. **Segurança**: FastAPI inclui recursos de segurança robustos, como autenticação e autorização, para proteger sua API.\n",
       "3. **Escalabilidade**: FastAPI é projetado para lidar com grande quantidade de tráfego e solicitações, tornando-a uma escolha popular para aplicações em escala.\n",
       "4. **Python nativo**: FastAPI é uma biblioteca Python, o que significa que você pode criar APIs em Python sem precisar aprender outros idiomas de programação.\n",
       "\n",
       "**Como funciona o FastAPI?**\n",
       "\n",
       "Aqui está um passo a passo básico para começar a usar o FastAPI:\n",
       "\n",
       "1. **Instalar o FastAPI**: Você pode instalar o FastAPI usando o comando `pip install fastapi` no seu ambiente Python.\n",
       "2. **Criar um arquivo de configuração**: Crie um arquivo de configuração (`main.py`) com as seguintes linhas:\n",
       "```python\n",
       "from fastapi import FastAPI\n",
       "\n",
       "app = FastAPI()\n",
       "```\n",
       "Isso cria uma instância do FastAPI chamada `app`.\n",
       "3. **Definir rotas**: Você pode definir rotas para sua API usando a sintaxe `@app.get(\"/\")`. Por exemplo:\n",
       "```python\n",
       "@app.get(\"/\")\n",
       "def read_root():\n",
       "    return {\"Message\": \"Hello World\"}\n",
       "```\n",
       "Isso define uma rota GET para o endereço `/` que retorna a string `\"Hello World\"`.\n",
       "4. **Rodar o servidor**: Você pode rodar o servidor FastAPI usando o comando `uvicorn main:app --host 0.0.0.0 --port 8000`. Isso começa o servidor e faz com que ele escute solicitações em `http://localhost:8000`.\n",
       "\n",
       "E pronto! Agora você tem um servidor de API rodando no FastAPI.\n",
       "\n",
       "Essa é uma visão geral básica do FastAPI. Se você tiver mais perguntas ou precisar de ajuda, sinta-se à vontade para perguntar!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython import display\n",
    "from io import StringIO\n",
    "\n",
    "prompt = LLMChatPrompt()\n",
    "\n",
    "question = \"O que é o FastAPI?\"\n",
    "response = StringIO()\n",
    "for stream in prompt.question_s(question=question):\n",
    "    response.write(stream)\n",
    "    display.display(display.Markdown(response.getvalue()), clear=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Claro! Vamos criar um exemplo de CRUD (Create, Read, Update, Delete) com o FastAPI.\n",
       "\n",
       "**Criando um CRUD com o FastAPI**\n",
       "\n",
       "Neste exemplo, vamos criar uma API que armazena informações de livros. A API terá as seguintes rotas:\n",
       "\n",
       "* `POST /livros`: criar um novo livro\n",
       "* `GET /livros`: recuperar todos os livros\n",
       "* `GET /livros/{id}`: recuperar um livro pelo ID\n",
       "* `PUT /livros/{id}`: atualizar um livro pelo ID\n",
       "* `DELETE /livros/{id}`: deletar um livro pelo ID\n",
       "\n",
       "**Código do CRUD**\n",
       "```python\n",
       "from fastapi import FastAPI, HTTPException\n",
       "from pydantic import BaseModel\n",
       "from uuid import UUID\n",
       "\n",
       "app = FastAPI()\n",
       "\n",
       "class Livro(BaseModel):\n",
       "    id: UUID\n",
       "    title: str\n",
       "    author: str\n",
       "\n",
       "livros = []\n",
       "\n",
       "@app.post(\"/livros\")\n",
       "async def create_livro(livro: Livro):\n",
       "    livro.id = UUID()\n",
       "    livros.append(livro)\n",
       "    return livro\n",
       "\n",
       "@app.get(\"/livros\")\n",
       "async def read_livros():\n",
       "    return livros\n",
       "\n",
       "@app.get(\"/livros/{id}\")\n",
       "async def read_livro(id: UUID):\n",
       "    for livro in livros:\n",
       "        if livro.id == id:\n",
       "            return livro\n",
       "    raise HTTPException(status_code=404, detail=\"Livro não encontrado\")\n",
       "\n",
       "@app.put(\"/livros/{id}\")\n",
       "async def update_livro(id: UUID, livro: Livro):\n",
       "    for i, existing_livro in enumerate(livros):\n",
       "        if existing_livro.id == id:\n",
       "            livros[i] = livro\n",
       "            return livro\n",
       "    raise HTTPException(status_code=404, detail=\"Livro não encontrado\")\n",
       "\n",
       "@app.delete(\"/livros/{id}\")\n",
       "async def delete_livro(id: UUID):\n",
       "    for i, livro in enumerate(livros):\n",
       "        if livro.id == id:\n",
       "            del livros[i]\n",
       "            return {\"message\": \"Livro deletado com sucesso\"}\n",
       "    raise HTTPException(status_code=404, detail=\"Livro não encontrado\")\n",
       "```\n",
       "**Explicação do código**\n",
       "\n",
       "Neste exemplo, criamos uma classe `Livro` que representa um livro com as propriedades `id`, `title` e `author`.\n",
       "\n",
       "A função `create_livro` cria um novo livro com um ID único gerado por meio da biblioteca `uuid`. O livro é armazenado em uma lista chamada `livros`.\n",
       "\n",
       "A função `read_livros` retorna a lista de livros armazenados.\n",
       "\n",
       "A função `read_livro` busca o livro pelo ID fornecido e retorna o livro se encontrado. Se não for encontrado, retorna um erro 404.\n",
       "\n",
       "A função `update_livro` atualiza o livro pelo ID fornecido com as novas propriedades. Se o livro for encontrado, atualiza-o; caso contrário, retorna um erro 404.\n",
       "\n",
       "A função `delete_livro` deleta o livro pelo ID fornecido da lista de livros. Se o livro for encontrado, deleta-o; caso contrário, retorna um erro 404.\n",
       "\n",
       "**Rodando o servidor**\n",
       "\n",
       "Para rodar o servidor, execute o comando:\n",
       "```\n",
       "uvicorn main:app --host 0.0.0.0 --port 8000\n",
       "```\n",
       "Agora você pode usar a API para criar, ler, atualizar e deletar livros!\n",
       "\n",
       "Por exemplo, você pode usar a ferramenta `curl` para testar as rotas:\n",
       "```bash\n",
       "# Criar um novo livro\n",
       "curl -X POST -H \"Content-Type: application/json\" -d '{\"id\": \"123e4567-e89b-12d3-a456-426614174000\", \"title\": \"Livro 1\", \"author\": \"Autor 1\"}' http://localhost:8000/livros\n",
       "\n",
       "# Ler todos os livros\n",
       "curl -X GET http://localhost:8000/livros\n",
       "\n",
       "# Ler um livro pelo ID\n",
       "curl -X GET http://localhost:8000/livros/123e4567-e89b-12d3-a456-426614174000\n",
       "\n",
       "# Atualizar um livro pelo ID\n",
       "curl -X PUT -H \"Content-Type: application/json\" -d '{\"id\": \"123e4567-e89b-12d3-a456-426614174000\", \"title\": \"Livro 2\", \"author\": \"Autor 2\"}' http://localhost:8000/livros/123e4567-e89b-12d3-a456-426614174000\n",
       "\n",
       "# Deletar um livro pelo ID\n",
       "curl -X DELETE http://localhost:8000/livros/123e4567-e89b-12d3-a456-426614174000\n",
       "```\n",
       "Espero que isso tenha ajudado! Se você tiver mais perguntas ou precisar de ajuda, sinta-se à vontade para perguntar."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "question = \"Poderia criar um CRUD?\"\n",
    "response = StringIO()\n",
    "for stream in prompt.question_s(question=question):\n",
    "    response.write(stream)\n",
    "    display.display(display.Markdown(response.getvalue()), clear=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
