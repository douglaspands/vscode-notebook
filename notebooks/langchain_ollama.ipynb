{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ollama + LangChain\n",
    "\n",
    "Usar a API do Ollama com o biblioteca LangChain.\n",
    "\n",
    "Como instalar:\n",
    "\n",
    "```sh\n",
    "pip install langchain-ollama\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uso basico de exemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "LangChain is an open-source library developed for building blockchain-agnostic data pipelines and workflows. It provides a simple, Pythonic API for interacting with various blockchains and decentralized applications (dApps).\n",
       "\n",
       "To break it down further:\n",
       "\n",
       "1. **Blockchain Agnosticism**: LangChain allows developers to interact with different blockchain platforms, such as Ethereum, Polygon, Solana, and more, using a unified interface. This means that you can write code once and deploy it on multiple blockchains without having to rewrite it for each specific platform.\n",
       "2. **Data Pipelines**: LangChain enables the creation of data pipelines that can ingest data from various sources, process it, and then export it to different destinations, such as databases or other blockchain platforms.\n",
       "3. **Workflows**: The library also provides support for building workflows that can execute a series of tasks or operations in a specific order. This is useful for automating complex processes, such as data validation, data transformation, or even decentralized application (dApp) functionality.\n",
       "\n",
       "Overall, LangChain aims to simplify the development and deployment of blockchain-based data pipelines and workflows, making it more accessible to developers and increasing adoption across various industries and use cases.\n",
       "\n",
       "How's that? Did I cover the basics of LangChain effectively?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "from IPython.display import Markdown\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "model = OllamaLLM(model=\"llama3.2\")\n",
    "\n",
    "chain = prompt | model\n",
    "\n",
    "response = chain.invoke({\"question\": \"What is LangChain?\"})\n",
    "Markdown(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uso basico em Português"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "from typing import Iterator\n",
    "from enum import StrEnum\n",
    "\n",
    "\n",
    "class LLMEnum(StrEnum):\n",
    "    LAMMA3_1 = \"llama3.1:8b\"\n",
    "    LAMMA3_2 = \"llama3.2:3b\"\n",
    "    LAMMA3_2_VISION = \"llama3.2-vision:11b\"\n",
    "    GEMMA2_2 = \"gemma2:2b\"\n",
    "    GEMMA2_9 = \"gemma2:9b\"\n",
    "    PHI3_5 = \"phi3.5:3.8b\"\n",
    "    LLAVA = \"llava:7b\"\n",
    "\n",
    "\n",
    "class LLMPrompt:\n",
    "    TEMPLATE = \"\"\"\n",
    "        Pergunta: {question}\n",
    "        \n",
    "        Resposta: Me explique com calma e passo a passo.\n",
    "    \"\"\"\n",
    "    MODEL = LLMEnum.LAMMA3_2\n",
    "    \n",
    "    def __init__(self):\n",
    "        prompt = ChatPromptTemplate.from_template(self.TEMPLATE)\n",
    "        model = OllamaLLM(model=self.MODEL)\n",
    "\n",
    "        self._chain = prompt | model\n",
    "\n",
    "    def question(self, question: str, stream: bool = False) -> str | Iterator[str]:\n",
    "        if stream is True:\n",
    "            return self._chain.stream({\"question\": question})\n",
    "        return self._chain.invoke({\"question\": question})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Entendendo LangChain**\n",
       "\n",
       "LangChain é uma biblioteca de Python open-source projetada para simplificar a criação de aplicativos de inteligência artificial. Foi criada em 2020 por uma equipe de engenheiros da OpenAI, e desde então tem sido amplamente utilizada no desenvolvimento de modelos de linguagem.\n",
       "\n",
       "**Como funciona LangChain?**\n",
       "\n",
       "LangChain é um framework que permite criar aplicativos de IA mais complexos e escaláveis. Aqui está uma explicação passo a passo sobre como ele funciona:\n",
       "\n",
       "### 1. **Instalação**\n",
       "\n",
       "Para usar LangChain, você precisa instalar a biblioteca Python. Pode ser feita através do pip (Python Package Installer):\n",
       "\n",
       "```bash\n",
       "pip install langchain\n",
       "```\n",
       "\n",
       "### 2. **Criando um modelo de IA**\n",
       "\n",
       "LangChain suporta uma variedade de modelos de linguagem, incluindo o modelo de IA GPT-3. Para usar LangChain com um modelo de IA, você precisa importá-lo e criar uma instância do objeto.\n",
       "\n",
       "```python\n",
       "from langchain.llms import Gpt3LLM\n",
       "\n",
       "llm = Gpt3LLM()\n",
       "```\n",
       "\n",
       "### 3. **Definindo a tarefa**\n",
       "\n",
       "LangChain permite definir a tarefa para a qual o modelo de IA será usado. Isso pode incluir desde resumir textos até gerar texto a partir de uma entrada.\n",
       "\n",
       "```python\n",
       "tarefa = \"resumir\"\n",
       "```\n",
       "\n",
       "### 4. **Executando a tarefa**\n",
       "\n",
       "Com a tarefa definida, você pode executá-la usando o método `execute()` do objeto LangChain.\n",
       "\n",
       "```python\n",
       "resumo = llm.execute(tarefa, texto=\"Este é um texto longo\")\n",
       "print(resumo)\n",
       "```\n",
       "\n",
       "### 5. **Processamento de resultados**\n",
       "\n",
       "LangChain permite processar os resultados da tarefa e extrair informações relevantes. Por exemplo, se a tarefa for resumir textos, você pode usar o método `get_summary()` para obter o resumo.\n",
       "\n",
       "```python\n",
       "resumo = llm.execute(tarefa, texto=\"Este é um texto longo\")\n",
       "print(resumo.get_summary())\n",
       "```\n",
       "\n",
       "**Conclusão**\n",
       "\n",
       "LangChain é uma biblioteca poderosa que permite criar aplicativos de IA mais complexos e escaláveis. Com essa explicação passo a passo, você deve ter uma boa ideia sobre como usar LangChain para criar seus próprios modelos de linguagem. Lembre-se de que LangChain é apenas uma ferramenta, e o sucesso da sua aplicação depende das suas habilidades em programação e desenvolvimento de software."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "prompt = LLMPrompt()\n",
    "\n",
    "question = \"O que é LangChain?\"\n",
    "response = prompt.question(question=question)\n",
    "Markdown(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Claro, vou explicar o que é LangChain de forma clara e detalhada.\n",
       "\n",
       "**O que é LangChain?**\n",
       "\n",
       "LangChain é uma biblioteca de código aberto para o desenvolvimento de chatbots e sistemas de linguagem natural. Ela permite criar modelos de linguagem avançados que podem entender e gerar texto com precisão e fluidez.\n",
       "\n",
       "**Como funciona LangChain?**\n",
       "\n",
       "Aqui está um passo a passo sobre como funciona LangChain:\n",
       "\n",
       "1. **Instalação**: O primeiro passo é instalar o LangChain no seu ambiente de desenvolvimento. Isso pode ser feito usando o comando `npm install langchain` ou `yarn add langchain`.\n",
       "2. **Configuração**: Após a instalação, você precisa configurar o LangChain para funcionar com o seu modelo de linguagem. Isso inclui especificar o tipo de modelo (por exemplo, BERT, RoBERTa, etc.) e suas configurações.\n",
       "3. **Preparação do texto**: Para que o LangChain possa entender o texto, você precisa prepará-lo para a análise. Isso pode incluir remover stopwords, remover palavras irrelevantes, normalizar os caracteres, etc.\n",
       "4. **Avaliação do texto**: Com o texto pronto, o LangChain pode avaliar sua complexidade e determinar qual é a resposta mais provável.\n",
       "5. **Gerador de texto**: Após a avaliação, o LangChain pode gerar um texto baseado na resposta antecipada.\n",
       "\n",
       "**Vantagens do LangChain**\n",
       "\n",
       "Algumas das vantagens do LangChain incluem:\n",
       "\n",
       "*   **Precisão alta**: O LangChain é capaz de entender e gerar texto com precisão e fluidez.\n",
       "*   **Flexibilidade**: O LangChain pode ser usado para criar chatbots, sistemas de linguagem natural, e outros aplicativos que envolvem a compreensão e geração de texto.\n",
       "*   **Facilidade de uso**: A biblioteca é fácil de usar e entender, mesmo para desenvolvedores com pouco conhecimento em linguagem natural.\n",
       "\n",
       "**Conclusão**\n",
       "\n",
       "Em resumo, o LangChain é uma poderosa biblioteca de código aberto que permite criar modelos de linguagem avançados. Ele oferece precisão alta, flexibilidade e facilidade de uso, tornando-o uma ferramenta valiosa para desenvolvedores de chatbots e sistemas de linguagem natural."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython import display\n",
    "from io import StringIO\n",
    "\n",
    "prompt = LLMPrompt()\n",
    "\n",
    "question = \"O que é LangChain?\"\n",
    "response = StringIO()\n",
    "for stream in prompt.question(question=question, stream=True):\n",
    "    response.write(stream)\n",
    "    display.display(display.Markdown(response.getvalue()), clear=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
